---
title: "Response to Reviewers for: Deep Learning does not Replace Bayesian Modeling"
author: "Breck Baldwin"
email: "breckbaldwin@gmail.com"
date: "10/15/2021"
output:
 bookdown::html_document2:
   includes:
     in_header: _html/ga.html 
     
bibliography: citations.bib
csl: american-medical-association.csl
link-citations: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	comment = NA
)
```

``` {echo=TRUE}
Reviewer: 1

Comments to the Author
An interesting paper with much to think on. It is noticeable that Deep Learning is now the chosen method in most fields even though it is computer science which dominates. This should perhaps be noted. It would be interesting to consider whether adopting Deep Learning in these fields is always appropriate or if Bayesian methods could offer a more appropriate alternative.
```

Author Response


R1:
This paper also demonstrates that deep learning is not the default tool when it is time for some reasoning in the scientific enterprise. If you are doing immunology/microbiology then Bayesian methods dominate. Environmental science, economics are a coin flip. While the choice of machine reasoning techniques should be entirely determined by the problem being addressed, researchers use what is popular and being talked about. Perhaps this paper gets Bayesian methods considered a bit more. 
:R1

Added comment

``` {echo=TRUE}
Reviewer: 2

Comments to the Author
The author provides a robust and necessary analysis of the interest around and funding dedicated to the areas of Bayesian inference and Deep Learning. The analysis yields interesting insights into the utilisation of these learning frameworks in a variety of fields. The article is concise, well written and I in my opinion is ready for publication as is.
```

``` {echo=TRUE}
Reviewer: 3

Comments to the Author
A thought-provoking paper, which with some refinement provides further evidence regarding the importance of computational statistics - often peddled as AI hype.

I have a series of questions & comments that I would welcome author response to.

Abstract: you comment “once computer science is subtracted”. How can you argue for this though, given that so much work in both deep learning and probabilistic modeling (though as below, please don’t assume these are different!) sits squarely in computer science! This feels, without full argument, like cherry-picking - and of course, in the subjects that are left (dominated by Stats & Applied Math) it’s inevitable that we find more usage of Bayesian methods (and probably more use of R as opposed to Python too!)

Google, Facebook, Microsoft and the gang are strong exponents of Bayes! Let’s remember that Bayesian Optimization sits at the core of hyper-parameter tuning for AlphaGoZero and (behind the scenes) for much model updating in Facebook.
```
### Abstract revised:

One could be excused for assuming that deep learning had or will soon usurp all credible work in reasoning, artificial intelligence and statistics, but like most 'meme' class broad generalizations the concept does not hold up to scrutiny. Memes don't generally matter since the experts will always know better but in the case of Bayesian software like Stan and PyMC3 even its developers and advocates bemoan the apparent dominance of deep learning as manifested in popular culture, breathtaking performance and most problematically from funding agency peer review that impacts our ability to further advance the field. The facts however do not support the assumed dominance of deep learning in science upon closer examination.

This letter simply makes the argument by the crudest of possible metrics, citation count, that once R2:the discipline of:R2 Computer Science is subtracted, Bayesian software accounts for nearly a third of research citations. Stan and PyMC3 dominate some fields, PyTorch, Keras and TensorFlow dominate others with lots of variation in between. Bayesian and deep learning approaches are related but very different technologies in goals, implementation and applicability with little actual overlap so this is not a surprise, R3: e.g., deep learning cannot bring the explainability of applied math/statistics and Bayesian methods do not scale to deep learning data sets.

While deep learning behemoths like Facebook and Google both use and support Bayesian efforts, the Bayesian packages scientists actually use are academic/volunteer efforts punching far above their weight class and they need financial support. It would behoove funders to fully understand the impact and role of Bayesian methods in resource allocation.:R3




```{echo=TRUE}

Introduction: There is a *huge* rise in the use of Bayesian Deep learning. Please do not fall into the trap of making DL look like some ‘other’ thing. It’s just a non/hyper-parametric model and as such we can do all the full Bayesian stuff - and many do! There are well-documented works on semi-structured HMC, Riemann MC and good old fashioned MCMC for use in big deep nets, let alone all the neat work on approximate Bayes, mainly using variational learning. Look at work on things like loss-calibrated Bayesian deep nets, Bayes DL, Bayesian autoencoding, ‘HamilTorch’ MCMC package for TensorFlow (https://adamcobb.github.io/journal/hamiltorch.html) etc. You just can’t ignore all this. (and please don’t forget all the fine folks doing {S,O,P}DiffEq learning using things like graph NNs and ResNets - lots of Bayes to please everyone in the ‘physics meets DL’ world).
```





Intro: you claim NSF dismissal of (eg) Stan - where is the evidence citation for this?

Intro: Bayesian software - myopic - more to Bayes than sampling (which is non-Bayesian in any case!). Lots of work in AI using approximate Bayes, ABC, VB and so on. Gaussian Processes are HUGE in their own right too - but no MCMC in sight normally. You really need to note that Gaussian Processes, and Neural Kernel methods, are *fully Bayesian* yet DL replacements for many applications.

Sect 3. I just do not buy that packages are a proxy for research work. As before, what about all the "Bayesian" dropout work in NNs etc. VAEs? BayesDL tends not to use packages.

So, this has the making of a really interesting commentary - but really needs to address these issues before it can be published. I feel you do Bayes in DL a disservice by commenting just on big package usage.
```


Reviewer: 4

Comments to the Author
The author presents a lightweight but thought-provoking exposition of the scientific literature, providing interesting evidence that can be used by grantees and funders alike to justify funding statistical science (in particular Bayesian statistics) alongside broader computer science techniques badged under "AI". I was slightly surprised not to see references to Bayesian deep learning, or work at the intersection of Bayesian statistics and deep learning (e.g. using DNN as an approximation to a likelihood function), to support the point around complementarity, but this is a minor point and not worth a revision to the paper. I do hope the paper provokes further debate on this important topic.


# 